%\documentclass[eng,final, oneside, printmode]{mgr} %mwbk
\documentclass[pl,final,oneside]{mgr} %printmode
\usepackage[polish]{babel}
\usepackage{polski}
\usepackage[OT4]{fontenc}
\usepackage[utf8]{inputenc}
\frenchspacing

\usepackage{cmap}
\linespread{1.1} %interlinia
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{longtable}
\usepackage{graphicx} 
\usepackage{geometry}
\usepackage{pgfplots}
\usepackage{multirow}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\pgfplotsset{compat=1.13}
\usepackage{bookmark}
\usepackage{listings}
\usepackage{indentfirst}
\setcounter{secnumdepth}{6}
\usepackage{textcomp}
\usepackage[nottoc,numbib]{tocbibind}

\setlength{\parindent}{30pt}

\setlength{\parskip}{0pt}

\bookmarksetup{
	open,
	addtohook={%
		\ifnum\bookmarkget{level}<1 %
		\bookmarksetup{bold}%
		\fi
	},
}

\hypersetup{%
	colorlinks = true,
	linkcolor  = black
}

%\newgeometry{tmargin=1.5cm, bmargin=1.5cm, lmargin=1.5cm, rmargin=1.5cm}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}
\lstset{language=Bash,
	basicstyle=\scriptsize\ttfamily,
	rulecolor=\color{black},
	upquote=true,
	numbersep=8pt,
	showstringspaces=false,
	breaklines=true,
	frameround=ftff,
	frame=single,
	belowcaptionskip=5em,
	aboveskip=1em,
	belowskip=1em,
}


%\renewcommand\lstlistingname{Dodatek A - spis listingów}
%\renewcommand\lstlistlistingname{{Dodatek A - spis listingów}}

\author{Rafał Dziwiński}
\title{Analiza możliwości i implementacja niezawodnego routingu w wirtualnym środowisku High Avability przy wykorzystaniu platformy Kubernetes oraz Calico.}

\engtitle{Analysis of possibilities and implementation of reliable routing in the virtual High Avability environment using the Kubernetes and Calico platforms.}
\supervisor{dr inż. Waldemar Grzebyk, KTT}
\field{Telekomunikacja}
\specialisation{Teleinformatyka i multimedia}

% \renewcommand*\thesection{\arabic{section}.} % zmiana numeracji sekcji 0.X -> X
\begin{document}

\maketitle
\tableofcontents

\chapter{Wstęp}
W dzisiejszych czasach... blablabla (wstęp i cel)
\chapter{Cel i zakres pracy}
Aspekt badawczy:
1. Opracowanie zakresu analizy oraz kryteriów oceny.
2. Analiza możliwości implementacji niezawodnego routingu w wirtualnym środowisku High Avability przy
wykorzystaniu platformy Kubernetes oraz Calico:
a) analiza sposobów implementacji niezawodnego routingu w środowisku wirtualnym,
b) porównanie rozwiązania zwirtualizowanego do fizycznego w oparciu o przyjęte kryteria.
Aspekt inżynierski:
1. Przygotowanie środowiska testowego wykorzystującego platformę Kubernetes oraz implementacja wirtualnych
routerów jako kontenerów przy pomocy aplikacji Docker.
2. Przygotowanie scenariuszy i procedur testowych.
3. Przeprowadzenie testów dotyczących czasu odzyskania pełnej funkcjonalności sieci w momencie awarii jednego
bądź wielu z jej elementów.
Zadania do wykonania:
1. Studia literaturowe.
2. Opracowanie zakresu analizy oraz kryteriów oceny.
3. Analiza możliwości implementacji niezawodnego routingu w wirtualnym środowisku High Avability przy
wykorzystaniu platformy Kubernetes oraz Calico:
a) analiza sposobów implementacji niezawodnego routingu w środowisku wirtualnym,
b) porównanie rozwiązania zwirtualizowanego do fizycznego w oparciu o przyjęte kryteria.
4. Przygotowanie środowiska testowego wykorzystującego platformę Kubernetes oraz implementacja wirtualnych
routerów jako kontenerów przy pomocy aplikacji Docker.
5. Przygotowanie scenariuszy i procedur testowych.
6. Przeprowadzenie testów dotyczących czasu odzyskania pełnej funkcjonalności sieci w momencie awarii jednego
bądź wielu z jej elementów.
7. Opracowanie wniosków z przeprowadzonej analizy i testów.
\chapter{Część teorytyczna}
BlaBlaBla
\section{Słownik pojęć}
\begin{description}
\item[Pod] najmniejsza możliwa do wdrożenia jednostka obliczeniowa zarządzana przez platformę Kubernetes.
\item[Node] maszyna robocza zawierająca usługi niezbędne do uruchamiania podów oraz zarządzania nimi.
\item[Master nod] maszyna robocza zapewniająca płaszczyznę sterowania klastra.
\item[Worker node] maszyna robocza zapewniająca zasoby sprzętowe dla tworzonych podów.
\item[Host endpoint] reprezentuje jeden bądź wiele fizycznych bądź wirtualnych interfjesów połączonych z hostem na którym uruchomione jest calico.
\item[Workload endpoints] reprezentuje zasób wirtualny podłączony do sieci Calico.
\item[Control plane] descriptioncontrol plane   aadsaef
\end{description}
\section{Konteneryzacja}
Konteneryzacja
Aby opisać w dobry sposób konteneryzację warto przypomnieć sobie jak było kiedyś. Pierwszą metodą wdrażania aplikacji było uruchomienie każdej z nich na osobnym fizycznym serwerze. Niestety, w tym przypadku nie ma możliwości wydzielenia zasobów dla danej aplikacji co powoduje problemy z jednoczesnym uruchomieniem wielu aplikacji. Może się zdażyć tak, że duże obciążenie na przykład przez atak DDoS spowoduje zatrzymanie pracy innych, zupełnie niezwiązanych ze sobą usług. Jedynym wyjściem jest uruchomienie aplikacji na osobnych fizycznych maszynach jednak generuje to szereg problemów. Między innymi:
\begin{itemize}
	\item Trudność w skalowaniu zasobów;
	\item zwiększenie ilości niewykorzystanych zasobów;
	\item duże koszty.
\end{itemize}
Aby temu zaradzić wprowadzono pojęcie wirtualizacji. Pozwala ona na uruchomienie wielu wirtualnych maszyn (VM) na jednym fizycznym serwerze. Pozwala to na izolację aplikacji co ma wpływ na stabilność w przypadku przeciążenia jednej z aplikacji oraz bezpieczeństwo. Kolejnym plusem jest możliwość lepszego zarządzania oraz skalowania zasobów sprzętowych.  Zmniejsza również koszt wdrożenia oraz utrzymania dzięki możliwości zastosowania jednego, wydajnego serwera oraz ograniczenie kosztu prądu. W tym przypadku każda z VM wirtualizuje fizyczne zasoby oraz sprzęt co również nie jest rozwiązaniem w stu procentach wydajnym.

Na pomoc przychodzi tak zwana konteneryzacja. Jest to rozwiązanie podobne do maszyn wirtualnych z tą różnica, że nie jest potrzebna wirtualizacja sprzętu oraz systemu operacyjnego. Kontener działa na jądrze systemu hosta oraz jego zasobach które nie są wirtualizowane. Oznacza to, że w przypadku wdrożenia aplikacji w kontenerze potrzebne jest jedynie dodanie do niego pakietów charakterystycznych dla tej aplikacji. Na przykład w momencie uruchomienia dystrybucji Fedora na fizycznym serwerze z Debianem kontener będzie zawierał jedynie pakiety charakterystyczne dla tej pierwszej dystrybucji. W ogromnych skrócie można powiedzieć, że jest to lżejsza wersja VM. Ponadto użycie kontenerów niesie za sobą wiele zalet:
\begin{itemize}
	\item Większa łatwość i wydajność tworzenia obrazu kontenera w porównaniu do użycia obrazu VM.
	\item Łatwiejszy rozwój aplikacji przez możliwość szybkiego budowania kolejnych obrazów.
	\item Możliwość łatwiej migracji między wieloma różnymi usługami w chmurze.
	\item Centralne monitorowanie oraz zarządzanie konternerami.
	\item Wydajniejsze oraz efektywniejsze zarządzanie zasobami sprzętowymi.
	
\end{itemize}
Różnicę między tradycyjnym wdrażaniem aplikacji a VM oraz kontenerami najlepiej obrazuje i podsumowuje poniższa grafika: 
\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{images/container_evolution}
	\caption{Ewolucja możliwości wdrażania aplikacji \cite{kube_doku}.}
	\label{fig:container_evolution}
\end{figure}
\section{Kubernetes}
Słowo Kubernetes wywodzi się z języka greckiego i oznacza sternika bądź pilota. Pierwsze linie kodu napisane przez Google Inc. pojawiły się już w 2004 roku. Platforma bazuje na wieloletnim doświadczeniu tej firmy oraz społeczności.

Jest to otwarta platforma służąca do zarządzania skonteneryzowanymi aplikacjami oraz usługami. Głównym jej celem jest ułatwienie i automatyzacja ich wdrożeń oraz konfiguracji. Obecnie społeczność oraz firmy stojące za rozwojem systemu są na tyle liczne, że z powodzeniem ta młoda platforma może być wdrażana w środowiskach produkcyjnych.

Jak wspomniano w poprzednim rozdziale najlepszym z rozwiązań do uruchamiania aplikacji są kontenery. W środowisku produkcyjnym, liczącym wiele dziesiątek bądź też setek kontenerów    ręczne zarządzanie nimi jest niewydajne i obarczone ryzykiem wielu błędów. Również w momencie awarii konieczna jest szybka reakcja na przykład poprzez utworzenie zastępczych kontenerów. W tym celu bardzo dobrym wyjściem jest zastosowanie tak zwanego orkiestratora jakim jest Kubernetes. Do wymienionych wyżej zalet możemy dołączyć:

\begin{itemize}
	\item Wykrywanie usług oraz load balancing;
	\item automatyczne montowanie wybranych typów pamięci takich jak pamięć lokalna bądź zasób sieciowy;
automatyczne przydzielanie zasobów sprzętowych dla każdego z kontenerów;
	\item możliwość samoczynnego restartowania kontenera w momencie wykrycia awarii;
	\item łatwe zarządzanie danymi poufnymi wykorzystywanymi w kontenerach oraz zarządzanie ich konfiguracją.
\end{itemize}

\subsection{Komponenty}
W momencie wdrażania systemu Kubernetes nie uruchamiana jest pojedyncza aplikacja a szereg połączonych ze sobą usług. Aby to zrobić potrzebny jest tak zwany klaster składający się z minimum dwóch węzłów (node):
\begin{itemize}
	\item Worker node na której uruchamiane są kontenery 
	\item Master node zarządzający worker nodami oraz kontenerami w klastrze.
\end{itemize}
Przykładowy klaster wraz z wyszczególnionymi nodami został zaprezentowany poniżej.
\begin{figure}[h!]]
	\centering
	\includegraphics[width=1\linewidth]{images/components-of-kubernetes}
	\caption{Przykładowy klaster wraz z komponentami \cite{kube_doku}.}
	\label{fig:components-of-kubernetes}
\end{figure}

\subsubsection{Master Node}
Jak już wspomniano master node służy do zarządzania klastrem oraz kontenerami. Podejmuje on decyzje o umieszczeniu danego poda w node oraz zarządza ich zasobami. Składa się z szeregu komponentów które zostały wypisane oraz opisane poniżej:
\begin{description}
	\item [kube-apiserver] komponent control plane kubernetesa odpowiedzialny za komunikację użytkownika z platformą. 
	\item [etcd] silnie spójny i wysoce dostępny, rozproszony magazyn w formie klucz-wartość. Zapewnia niezawodny sposób przechowywaniakonfiguracji kubernetesa. Jeden z ważniejszych komponentów któremu konieczne jest zapewnienie nadmiarowości. Aby móc tolerować awarię jednego z modułów konieczne jest stworzenie klastra z ilością trzech.
	\item [kube-scheduler] komponent który w momencie wykrycia utworzenia nowego poda przypisuje go do odpowiedniego node bądź wielu. Decyzje te podejmuje na podstawie wymagań dotyczących zasobów/sieci poda oraz obciążenia obecnych workerów w klastrze. 
	\item [kube-controller-manager] komponent służący do uruchamiania kontrolerów. (https://kubernetes.io/docs/concepts/architecture/controller/) Każdy z kontrolerów jest osobnym procesem, lecz aby zwiększyć wydajnośc kompilowane są do jednego pliku binarnego. Zarządza on następującymi kontrolerami:
	\item [node controller] odpowiada za rekacje w momencie awarii jednego z node.
	\item [replication controller] zapewnia nadmiarowość podów w klastrze.
	\item [endpoints controller] zapewnia spójność usług oraz podów
	\item [service account and token controllers] tworzy użytkowników oraz zapewnia dostęp do API.
	\item [cloud-controller-manager] zapewnia integrację z siecią dostawcy usług chmurowych. 
\end{description}

\subsubsection{Node}
Poza kluczowymi komponentami wymienionymi w rozdziale wyżej które występują tylko w master node istnieją również dwa uruchamiane także na worker node.
\begin{itemize}
	\item kubelet - komponent którego zadaniem jest uruchamianie kontenerów wewnątrz podów. 
	\item kube-proxy - zarządza regułami sieciowymi w node pozwalającymi na komunikację podów z zasobami wewnątrz oraz poza klastrem. Jeżeli jest taka możliwość używa on zasoby oraz mechanizmy filtracji pakietów hosta.
	\item Container Runtime - komponent uruchamiający kontenery w klastrze. Na przykład: Docker, Containerd cri-o, bądź inny wspierany.
\end{itemize}



\section{Calico}
Calico jest to oprogramowanie wykorzystywane do zapewnienia sieci oraz jej bezpieczeństwa w środowiskach zwirtualizowanych oraz skonteneryzowanych. Jest ono otwarte i może być wykorzystywane przez wiele systemów takich jak OpenStack, OpenShift, Docker EE oraz Kubernetes. Głównymi założeniami twórców było stworzenie rozwiązania skalowalnego w środowiskach opartych o rozwiązania chmurowe oraz zapewnienie wydajności zbliżonej do rozwiązań oparty o jądro linuxa. Największymi zaletami Calico są:
\begin{itemize}
	\item Bezpieczeństwo - Użycie modelu “zero trust network” - wszystko co nie jest dozwolone jest zakazane.
	\item Wydajność - Calico korzysta z wbudowanych w jądro linuxa, wysoko zoptymalizowanych rozwiązań przekazywania oraz filtracji pakietów. W większości przypadków zbędne jest użycie funkcji enkapsulujących oraz dekapsulujących.
	\item Skalowalność - Osiągnięto ją dzięki wykorzystaniu najlepszych wzorców projektowych dla środowisk zwirtualizowanych oraz najpopularniejszych i najlepiej sprawdzonych protokołów sieciowych. Calico w cyklu testów deweloperskich jest uruchamiane na klastrze posiadającym wiele tysięcy node’s. 
	\item Uniwersalność - Poza zapewnieniem sieci dla systemów takich jak Kubernetes możliwe jest również wykorzystywanie w klasycznych maszynach wirtualnych a także interfejsach fizycznych.
\end{itemize}
\subsection{Host endpoints}
Calico poza możliwością definiowania punktów końcowych dla podów, maszyn wirtualnych bądź kontenerów umożliwia również tworzenie endpoints dla fizycznych interfejsów. Tak samo host endpoints może zawierać etykiety które znajdują się w tej samej przestrzeni nazw co workload endpoints. Calico nie wspiera konfiguracji adresów IP bądź konfiguracji w oparciu na adresach MAC interfejsów, zatem konieczna jest ich konfiguracja przez sieć podkładową naszego systemu.

Calico rozróżnia workload endpoints od host endpoints dzięki konfigurowalnym prefiksom. Dzięki polu  InterfacePrefix w konfiguracji modułu Felix możemy zdecydować, że punkty końcowe zaczynające się od danej wartości będą traktowane jako interfejsy workload. Pozostałe natomiast oznaczać będą interfejsy hosta.

Calico domyślnie blokuje cały ruch między workload interfaces, pozwalając tylko na ruch gdy interfejs jest znany oraz skonfigurowane są reguły bezpieczeństwa. W przypadku host endpoints Calico jest bardziej pobłażliwe. Monitoruje tylko ruch między zdefiniowanymi interfejsami, natomiast nie analizuje wszelkiego innego.

Możemy zastosować reguły bezpieczeństwa w kontekście punktów końcowych hosta dla trzech rodzajów ruchu:
\begin{itemize}
	\item zakończonego lokalnie,
	\item przekazywanego między host endpoints
	\item przekazywanego pomiędzy host endpoint a workload endpoint.
\end{itemize}
Przykład zastosowania wszystkich trzech rodzajów ruchu zaprezentowano na grafice poniżej;

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{images/bare-metal-packet-flows}
	\caption{Schmat pokazujący działanie calico w kontekście host endpoints \cite{calico_doku}.}
	\label{fig:bare-metal-packet-flows}
\end{figure}


\chapter{Aspekt inżynierski [temp]}
Jakiś wstęp że uruchomiono klaster itp itd co będzie robione.

\section{Uruchomienie klastra}
W celu realizacji pracy magisterskiej stworzono klaster Kubernetes'a składający się z x node. W celu symulacji maszyn fizycznych wykorzystano narzędzie KVM. Ze względu na ograniczenia sprzętowe klaster składa się tylko z jednego master node. Każdy z node to osobna maszyna wirtualna połączona ze sobą przy pomocy sieci izolowanej. Dodatkowo każdy z node posiada adres publiczny w celu zapewnienia dostępu do internetu. Systemem operacyjnym dla node jest Ubuntu 18.04.3 LTS. Reasumując w celu stworzenia klastra wykorzystano maszyny wirtualne zgodnie z tabelą \ref{tab:kvm_node}.

\begin{table}[h!]
	\begin{tabular}{lllllllll}
		Nazwa hosta   & Adres IP lokalny & Adres IP publiczny &  &  & CPU    & RAM  & Dysk &  \\
		k8s-01-master & 10.10.1.1        & 79.110.197.188     &  &  & 4 Core & 4 GB & 40GB &  \\
		k8s-02-worker & 10.10.1.2        & 79.110.197.189     &  &  & 2 Core & 2 GB & 40GB &  \\
		k8s-03-worker & 10.10.1.3        & 79.110.197.190     &  &  & 2 Core & 2 GB & 40GB & 
	\end{tabular}
	\label{tab:kvm_node}
	\caption{Lista node klastra}
\end{table}

W celu uruchomienia klastra wykorzystano narzędzie kubeadm wspierające nas w jego tworzeniu. Dzięki temu uruchomienie Kubernetesa staje się szybsze bez uszczerbku na wydajności. Dużym atutem tego narzędzia jest możliwość bardzo szybkiego dodania kolejnego worker node w razie potrzeby. Kolejne kluczowe kroki wraz z komendami zostały opisane poniżej.

\subsection{Wyłączenie nftables}
Wyłączenie nftables. Kubernetes wykorzystuje iptables i jest przygotowany do pracy z tym modułem. Najnowsze dystrybucje takie jak Debian Buster, Ubuntu 19.04, Fedora 29 wykorzystują nowsze narzędzie nftables. Należy pamiętać aby iptables nie wykorzystywało nftables jako backend. W przypadku wykorzystania Ubuntu 18.04.03 LTS można ten krok pominąć jednak warto mieć to na uwadze.
\begin{lstlisting}[language=Bash]
sudo -i
update-alternatives --set iptables /usr/sbin/iptables-legacy
update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
update-alternatives --set arptables /usr/sbin/arptables-legacy
update-alternatives --set ebtables /usr/sbin/ebtables-legacy
\end{lstlisting}

\subsection{Aktualizacja systemu oraz instalacja niezbędnych paczek}
Przed instalacją pakietów konieczna jest aktualizacja systemu oraz dodanie repozytorium Kubernetesa.
\begin{lstlisting}[language=Bash]
sudo -i
apt-get update && apt upgrade -y
apt-get install -y apt-transport-https ca-certificates curl software-properties-common
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
apt-get update
\end{lstlisting}

Następnie należy zainstalować system kontenerów czyli Docker. Wersja Kubernetesa 1.17 wymaga go w maksymalnie wersji 19.03 i taka też zostanie zainstalowana. Warto pamiętać o blokadzie automatycznej aktualizacji w celu zapewnienia stabilności klastra.

\begin{lstlisting}[language=Bash]
sudo -i
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/docker.list
deb https://download.docker.com/linux/ubuntu bionic stable
EOF
apt-get update && apt-get install \
docker-ce=5:19.03.4~3-0~ubuntu-$(lsb_release -cs) \
docker-ce-cli=5:19.03.4~3-0~ubuntu-$(lsb_release -cs)
apt-mark hold docker-ce docker-ce-cli
\end{lstlisting}	
Po instalacji Docker'a należy zmienić domyślny sterownik cgroup z cgroupfs na rekomendowany systemd. 
\begin{lstlisting}[language=Bash]
sudo -i
cat > /etc/docker/daemon.json <<EOF
{
"exec-opts": ["native.cgroupdriver=systemd"],
"log-driver": "json-file",
"log-opts": {
"max-size": "100m"
},
"storage-driver": "overlay2"
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

systemctl daemon-reload
systemctl restart docker
\end{lstlisting}
Ostatnim krokiem jest instalacja kubeadm oraz pakietów Kubernetesa. Przed uruchomieniem Kubernetesa konieczne jest wyłączenie partycji SWAP.
\begin{lstlisting}[language=Bash]
sudo swapoff -a
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
\end{lstlisting}

\subsection{Stworzenie master oraz worker node}
Jak już wspomniano do instalacji klastra wykorzystano narzędzie kubeadm pozwalające przyśpieszyć wiele procesów instalacyjnych. Najważniejszymi poleceniami wspomnianej aplikacji jest kubeadm init oraz kubeadm join. Najpierw zostało opisane polecenie inicjalizujące control plane klastra tj. kubeadm init. Posiada ono szereg argumentów pozwalających na dostosowanie klastra do własnych potrzeb. Najważniejsze opisano poniżej:
\begin{description}
	\item[control-plane-endpoint] Przydatne szczególnie w komecie rozbudowy klastra o kolejny master node. Umożliwia ustawienie wspólnego punktu końcowego dla wszystkich węzłów control plane.
	\item[pod-network-cidr] Określa pulę adresów IP wykorzystywanych dla sieci z podami.
	\item[apiserver-advertise-address] Domyślnie kubeadm używa interfejsu przypisane do bramy domyślnej jako adresu docelowego serwera API master node.
\end{description}
Zatem polecenie inicjalizujące control plane klastra to: 
\begin{lstlisting}[language=Bash]
kubeadm init --control-plane-endpoint 10.10.1.1 --pod-network-cidr 192.168.0.0/18 --apiserver-advertise-address 10.10.1.1
\end{lstlisting}
Jeżeli wszystko zostanie sprawnie zainstalowane będziemy o tym poinformowani stosownym komunikatem. Zawiera on również informację o możliwości dodania kolejnego node. Jego fragment został zaprezentowany poniżej:

\begin{lstlisting}[language=Bash]
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

kubeadm join 10.10.1.1:6443 --token xx.xxx \
--discovery-token-ca-cert-hash sha256:xxxx \
--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.10.1.1:6443 --token xx.xxx \
--discovery-token-ca-cert-hash sha256:xxxx
\end{lstlisting}

Ostatnim krokiem pracy z master node jest instalacja Calico. Odbywa się to w kolejnych etapach:
\begin{enumerate}
	\item Pobranie pliku konfiguracyjnego ze strony https://docs.projectcalico.org/v3.11/manifests/calico.yaml
	\item Edycja wartości CALICO\_IPV4POOL\_CIDR zgodnie z zadeklarowaną wcześniej pulą.
	\item Uruchomienie pliku konfiguracyjnego. \textit{kubectl apply -f calico.yaml}
\end{enumerate}

Aby możliwe było tworzenie kontenerów należy dodać worker node. Dzięki zastosowaniu kubeadm możliwe jest to za pomocą jednej komend.
\begin{lstlisting}{language=Bash}
kubeadm join 10.10.1.1:6443 --token xx.xxx --discovery-token-ca-cert-hash sha256:xxxx
\end{lstlisting}
W przypadku braku tokenu możliwe jest jego stworzenie wykonując poniższe polecenie na master node:
\begin{lstlisting}[language=Bash]
kubeadm token create
\end{lstlisting}
natomiast hash certyfikatu zostanie wyświetlony poleceniem:
\begin{lstlisting}[language=Bash]
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null \
| openssl dgst -sha256 -hex | sed 's/^.* //'
\end{lstlisting}

//tu będzie tabela z kubectl get nodes
\begin{lstlisting}[language=Bash]
rdziwinski@k8s-01-master:~$ kubectl get node -o wide
NAME            STATUS   ROLES    VERSION   INTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k8s-01-master   Ready    master   v1.17.0   10.10.1.1     Ubuntu 18.04.3 LTS   4.15.0-72-generic   docker://19.3.4
k8s-02-worker   Ready    <none>   v1.17.0   10.10.1.2     Ubuntu 18.04.3 LTS   4.15.0-72-generic   docker://19.3.4
\end{lstlisting}


Master i worker node są gotowy do pracy. Jak widać kolumna INTERNAL-IP zawiera błędne dane. Aby to zmienić, na obu node należy edytować plik konfiguracyjny dodając argument --node-ip=10.10.1.x do zmiennej KUBELET\_CONFIG\_ARGS:
\begin{lstlisting}[language=Bash]
vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
\end{lstlisting}
oraz zresetować procesy
\begin{lstlisting}[language=Bash]
systemctl daemon-reload
systemctl restart kubelet
\end{lstlisting}




Przy tworzeniu wykorzystano narzędzie kubeadm.
  
  


Put your code here.



\subsection{Uruchomienie Calico}


\subsection{Konfiguracja (pody, network policy etc)}



\chapter{Aspekt badawczy [temp]}

\begin{thebibliography}{99}
	\bibitem{kube_doku} 
	Dokumentacja projektu Kubernetes,
	\\ \textit{https://kubernetes.io/docs/home/}
	
	\bibitem{calico_doku}
	Dokumentacja projektu Calico \\
	\textit{https://docs.projectcalico.org/v3.10/introduction/}

	\bibitem{kube_in_action} 
	Kubernetes in Action, Manning Publications 2017 \\
	\textit{ISBN: 9781617293726}
	
	\bibitem{kube_up_and_run} 
	Kubernetes: Up and Running, O'Reilly Media, Inc. 2017 \\
	\textit{ISBN: 9781491935675}
	
	\bibitem{docker_fundamentals}
	Learn Docker - Fundamentals of Docker 18.x, Packt Publishing 2018\\
	\textit{ISBN: 9781788997027}

\end{thebibliography}
\end{document}

